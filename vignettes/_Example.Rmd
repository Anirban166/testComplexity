---
title: "Example vignette"
author: "Anirban Chetia"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Example vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<p style="font-family: times, serif; font-size:25pt; font-style: italic" align="center">
Welcome to testComplexity
</p>

-------------------------------------------------------------------------------------------------

This vignette is written to give the user a general introduction to the features and functionality of the package, via a set of textual illustrations and one example.

## Brief Overview

As can be guessed from the name itself, this package provides a suite of functions to test the asymptotic complexities of R functions/algorithms, where the term "complexity" here refers to the *time* complexity, as per the initial idea. (and hence a relevant logo with the stopwatch was designed early on!) 

### Features

Going by the base objectives stated [here](https://github.com/rstats-gsoc/gsoc2020/wiki/Asymptotic-complexity-testing#details-of-your-coding-project), this package provides:

- A function for quantifying the empirical time complexity of any R expression. 
- A time complexity classifying function which operates on the result (a data frame) of the previous function.
- Testing functions, to test for an expected time complexity class directly.

In addition to these, a few more features have been implemented:
- Memory variants for the time quantifier and complexity classifier.
- Plotting functions for both time and memory cases.
- A complexity classifier for a generalized use in asymptotic trend classification among any two parameters, not being restricted to time/memory cases.

-------------------------------------------------------------------------------------------------

Based on whether the user has an idea of the complexity class the input algorithm falls in, two scenarios are possible:

- The complexity class of the algorithm is *not known*, or diagnostic tests to estimate it have not been performed yet. 
In this case, the user could directly use the [quantifiers](https://anirban166.github.io/testComplexity/articles/Example.html#quantifiers) and [complexity classifiers](https://anirban166.github.io/testComplexity/articles/Example.html#complexity-classifiers) to obtain the asymptotic complexity class. 
- The complexity class is *known* via theoretical proof/empirical observation. 
For this case, the user can directly proceed to use the [testers](https://anirban166.github.io/testComplexity/articles/Example.html#testers) for verifying the theoretically-derived/empirically-obtained result, apart from using the previous method.

Note that the worst-case complexity class is currently set to quadratic. Cubic and exponential classes were initially thought of to be included, but since a $O(N^2)$ approach is the limit in practically used algorithms, subsequently higher complexity classes were discarded.

If the estimated complexity class turns out to be quadratic, its an indicator that the algorithm could do with a better approach, so as to minimize the computational resources (time/memory).

In such situations, the user may as well want to think of ways to reduce the time complexity, in which case the aforementioned functions will be helpful to continuously give feedback (via re-runs) for each improvement that the user thinks could make a difference in the complexity trend. 
This in turn could potentially lead to improvements in code performance, if such feedback-based optimizations are implemented in order to achieve better computational efficiency.

-------------------------------------------------------------------------------------------------

## Examples

To demonstrate the functionality of our package, I'll be taking the function [`PeakSegPDPA`](https://github.com/tdhock/PeakSegOptimal/blob/master/R/PeakSegPDPA.R) from the [`PeakSegOptimal`](https://github.com/tdhock/PeakSegOptimal) package as an example for each of the functions stated below. Note that it is empirically observed to be a log-linear time and memory algorithm, or the expected complexity classes for both time and memory cases is $O(NlogN)$ for input size `N`. This fact here is essential to know before-hand as you'll see<sup>1</sup> that testComplexity's functions obtain the same result, demonstrating its accuracy.

## Function Categories

Based on the different aspects of functionality, the functions in this package can be grouped into four different categories:

### [Quantifiers](https://anirban166.github.io/testComplexity/reference/index.html#section-quantifiers): 
These are the quantifying functions which compute the empirical benchmarks for runtimes (via the `asymptoticTimings` function) and memory allocations (via the `asymptoticMemoryUsage` function) against allotted data sizes.
For user convenience, a few points have been taken note of with respect to the parameters:

- The user-supplied algorithm is ideally accepted as an expression, which would be a function that scales with `N` (as a parameter) so as to comply with the general notation of `N` resembling the input size when we refer to asymptotic complexity classes (such as $O(N)$ denoting the linear variant, or a linear asymptotic trend in `N`). The values which `N` can take are seperately specified by the vector/set of user-provided values for the argument `data.sizes`. 

- Parameters such as `max.bytes` and `max.seconds` are introduced to put a desirable restriction on the time invested and memory allocated respectively, thereby saving resources (time/memory) for the otherwise possible further computation once the limit has been breached. 
The default values have been set taking into account the correct prediction for different test-cases, and should enact as an appropriate threshold for most algorithms. 

The only requirement for the user is to be able to identify the parameter in his/her *own* function/algorithm which can scale asymptotically for different data sizes, or in simple terms the parameter which can contain different values and the function's resource utilization varies depending on that. (which is fairly easy to figure out and is expected from the user)

To demonstrate the working, let us consider the example function `PeakSegOptimal::PeakSegPDPA()` and its parameters:

- `count.vec`: It should be an integer vector of count data.
- `weight.vec`: Its a numeric vector of positive weights, which would be computed via `count.vec` automatically from the default assignment. (doesn't need to be specified)
- `max.integer`: Maximum number of integer segments, which should be >=2 in value.

From the list of parameters above, the `count.vec` parameter is the one which when adjusted with higher values results in greater resource (time/memory) consumption, scaling asymptotically. Now in accordance with the first point mentioned above, we need to have that input vector of data as a function of `N`, which is relatively straight-forward depending on what type of data the function operates on. As per the function's description, it states that the algorithm computes optimal changepoint models using the *Poisson* likelihood for non-negative count data, which means we need to create a vector of poisson data in `N`, which is as simple as:
```r
data.vec = rpois(N, 1)
```
Now we simply emplace that function with its parameters for our expression argument in `asymptoticTimings()` and `asymptoticMemoryUsage()`, along with suitable data sizes to obtain the quantified timings/memory allocations respectively:
```r
df.time <- asymptoticTimings(e = PeakSegOptimal::PeakSegPDPA(count.vec = data.vec, max.segments = 3L), data.sizes = 10^seq(1, 4, by = 0.1))
data.table(df.time)
#>       Timings Data sizes
#>  1:    248701         10
#>  2:    120302         10
#>  3:    125701         10
#>  4:    133301         10
#>  5:    146500         10
#> ---                     
#> 696: 405597501      10000
#> 697: 408335001      10000
#> 698: 338544401      10000
#> 699: 404081901      10000
#> 700: 399575501      10000
```
```r
df.memory <- asymptoticMemoryUsage(PeakSegOptimal::PeakSegPDPA(count.vec = data.vec, max.segments = 3L), data.sizes = 10^seq(1, 4, by = 0.1))
data.table(df.memory)
#>    Memory usage Data sizes
#> 1:         6256   10.00000
#> 2:         7024   12.58925
#> 3:         7432   15.84893
#> 4:         8560   19.95262
#> 5:         9496   25.11886
#> --- 
#> 25:       447792 2511.88643
#> 26:       562336 3162.27766
#> 27:       706512 3981.07171
#> 28:       887792 5011.87234
#> 29:      1116240 6309.57344
```
If the seperate use of `N` in the step-wise argument construction for the above demonstration seems confusing, one can simply pass the parameters directly like `count.vec = rpois(N, 1)` (avoiding use of `data.vec`) or just `rpois(N, 1)` (passing arguments in order). 

Now that we have obtained our benchmarked data (consisting of runtimes and memory allocations) in a convienent `data.frame` format, we can proceed to find out the complexity class as demonstrated below. 

### [Complexity Classifiers](https://anirban166.github.io/testComplexity/reference/index.html#section-complexity-classifiers):
These are the complexity classification functions which internally use different stochastic procedures (such as cross-validation on different glm models, as derived from GuessCompx) to compute the complexity class for the user-provided algorithm via a best model match. Using them is relatively quite simple, as one just needs to pass the data frame obtained from the quantifiers to the appropriate complexity classifier. 

As an example, consider the data frames we obtained above for our function `PeakSegOptimal::PeakSegPDPA()` and pass them onto the complexity classifiers `asymptoticTimeComplexityClass()` and `asymptoticMemoryComplexityClass()` for time and memory cases respectively:
```r
asymptoticTimeComplexityClass(df.time)
#> [1] "loglinear"
```
```r
asymptoticMemoryComplexityClass(df.memory)
#> [1] "loglinear"
```
The resulting complexity classes turned out to be log-linear for both time and memory cases, (coincidentally for this function they are the same) which are exactly<sup>1</sup> the expected complexity classes. 

Note that you can chain the quantifiers with them, or call them and operate on their returned data frames directly:
```r
asymptoticTimeComplexityClass(asymptoticTimings(e = PeakSegOptimal::PeakSegPDPA(count.vec = data.vec, max.segments = 3L), data.sizes = 10^seq(1, 4, by = 0.1)))
#> [1] "loglinear"
asymptoticMemoryComplexityClass(asymptoticMemoryUsage(PeakSegOptimal::PeakSegPDPA(count.vec = data.vec, max.segments = 3L), data.sizes = 10^seq(1, 4, by = 0.1)))
#> [1] "loglinear"
```

Furthermore, if the user wants to classify the asymptotic trend between two custom parameters (which are not subject to being only runtimes/memory-allocations), they can use the `asymptoticComplexityClass()` function which as intended, computes the complexity class based on the trend followed between any two parameters of user-provided data which are contained in a data frame. 

The function accepts a data frame along with the two columnar parameters which the user needs to specify (as to which designates the output metric and which designates the data sizes, or something to relatively measure the trend against).

Note that since the computed data is already prevalent in the specified output parameter column of the data frame, we don’t need to use our quantifying functions or perform any benchmarks.

As a recurring example, consider passing the column names `Timings` and `Data sizes` along with the data frame returned by the time quantifier: (it consists of those two columns)
```r
asymptoticComplexityClass(df, output.size = "Timings", data.size = "Data sizes")
#> [1] "loglinear"
```
As expected, it classifies the correct complexity just like `asymptoticTimeComplexityClass()` did, as because the complexity classification procedure is same, its just that the parameters are customizable, which leads to a wider use-case scenario than just classifying the trend for time/memory cases.

